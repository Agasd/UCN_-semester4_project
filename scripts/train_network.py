# -*- coding: utf-8 -*-
"""5 minute  using 60 big

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V3makkJADY3F5mcn2T1YxaD_U-MLcfFI
"""

import random


import pandas as pd
import pickle
import numpy as np
import tensorflow as tf
from keras import Sequential
from keras.callbacks import TensorBoard, ModelCheckpoint
from keras.layers import LSTM, Dropout, BatchNormalization, Dense
from sklearn import preprocessing
from collections import deque
import datetime as dt
from sklearn.model_selection import train_test_split

SEQUENCE = 30  
FUTURE_PERIOD_PREDICT = 1 
TARGET_CURRENCY = "ETH-USD"
BATCH_SIZE = 2048
EPOCHS = 45

def join_data(datasets):
    PATH = "/content/drive/My Drive/Colab Notebooks/"
    main_df = pd.DataFrame()
    for dataset in datasets:

        dataset = dataset.split('.csv')[0]
        dataset_file = f'{PATH + dataset}.csv'
        df = pd.read_csv(dataset_file) 

        df.rename(columns={"close": f"{dataset}_close", "volume": f"{dataset}_volume"}, inplace=True)

        df.set_index("time", inplace=True) 
        df = df[[f"{dataset}_close", f"{dataset}_volume"]]  

        if len(main_df) == 0:  
            main_df = df 
        else:
            main_df = main_df.join(df)
    return main_df


def classify(current, future):
    if float(future) > float(current): 
        return 1
    else:
        return 0


def preprocess_df(df):
    df = df.drop("future", 1)
    df = df.drop("index", 1)
    df.dropna(inplace=True)

    for col in df.columns: 
        if col != "target" and col != "future_timestamp" and col != "time":
            df[col] = df[col].pct_change()
            df.dropna(inplace=True)
            print(df.columns)
            print(col)
            df[col] = preprocessing.scale(df[col].values) 
    df.dropna(inplace=True)
    
    sequential_data = [] 
    prev_days = deque(
        maxlen=SEQUENCE) 
    for i in df.values:
        prev_days.append([n for n in i[:-2]]) 
        if len(prev_days) == SEQUENCE: 
            sequential_data.append([np.array(prev_days), i[-2], i[-1]])  # append (feature,label timestamp, label)
            print(i[0])
    buys = []
    sells = []
    for seq, last, target in sequential_data:
        cont_loop = True
        temp = np.empty((len(seq), len(seq[0]) - 1))
        for i in range(0, len(seq) - 1):
            if int(seq[i, 0] + 60) < int(seq[i + 1, 0]):
                cont_loop = False
                print("seq error.: " + str(seq[i, 0] + 60) + " < " + str(seq[i + 1, 0]))
            if int(seq[len(seq) - 1, 0] + 60 * FUTURE_PERIOD_PREDICT) < int(last):
                cont_loop = False
                print("final error.: " + str(seq[len(seq) - 1, 0] + 60) + " < " + str(last))
            temp[i] = seq[i, 1:]
        temp[len(temp) - 1] = seq[len(seq) - 1, 1:]
        if cont_loop:
            print("LÖLÖ")
            if target == 0:
                sells.append([temp, target])
            elif target == 1:
                buys.append([temp, target])
        print(last)

    random.shuffle(buys)
    random.shuffle(sells)

    lower = min(len(buys), len(sells))
    
    buys = buys[:lower] 
    sells = sells[:lower]  

    sequential_data = buys + sells
    random.shuffle(sequential_data)

    X = []
    y = []

    for seq, target in sequential_data:
        X.append(seq)
        y.append(target)

    return np.array(X), y

def sort_data(df):
    times = sorted(df.index.values)
    last_30pct = sorted(df.index.values)[-int(0.3 * len(times))]
    validation_df = df[(df.index >= last_30pct)]
    validation_df.dropna(inplace = True)
    df = df[(df.index < last_30pct)]

    df = df.set_index('time').reindex(np.arange(df['time'].min(), df['time'].max(), 60)) \
        .fillna(np.nan) \
        .reset_index()

    df.ffill(limit=1,inplace = True)
    df.dropna(inplace = True)
    validation_df.dropna(inplace = True)


    df.reset_index(level=0, inplace=True)
    validation_df.reset_index(level=0, inplace=True)


    df['future_timestamp'] = df["time"].shift(-FUTURE_PERIOD_PREDICT)
    df['future'] = df[f'{TARGET_CURRENCY}_close'].shift(-FUTURE_PERIOD_PREDICT)
    df['target'] = list(map(classify, df[f'{TARGET_CURRENCY}_close'], df['future']))
    df.dropna(inplace=True)


    validation_df['future_timestamp'] = validation_df["time"].shift(-FUTURE_PERIOD_PREDICT)
    validation_df['future'] = validation_df[f'{TARGET_CURRENCY}_close'].shift(-FUTURE_PERIOD_PREDICT)
    validation_df['target'] = list(map(classify, validation_df[f'{TARGET_CURRENCY}_close'], validation_df['future']))
    validation_df.dropna(inplace=True)

    print(df.head())
    print("\n ---------------")
    print(df.tail())
    print("\n ---------------")
    print("\n ---------------")
    print(validation_df.head())
    print("\n ---------------")
    print(validation_df.tail())
    train_x, train_y = preprocess_df(df)
    test_x, test_y = preprocess_df(validation_df)
    return train_x, test_x, train_y, test_y

def create_network(input_shape):
    model = Sequential()
    model.add(LSTM(256, input_shape=(input_shape), return_sequences=True))
    model.add(Dropout(0.2))
    model.add(BatchNormalization())

    model.add(LSTM(256, return_sequences=True))
    model.add(Dropout(0.2))
    model.add(BatchNormalization())

    model.add(LSTM(256))
    model.add(Dropout(0.2))
    model.add(BatchNormalization())

    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.2))

    model.add(Dense(2, activation='softmax'))

    opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)

    # Compile model
    model.compile(
        loss='sparse_categorical_crossentropy',
        optimizer=opt,
        metrics=['accuracy']
    )
    return model

from google.colab import drive
drive.mount('/content/drive')

main_df = join_data(["BTC-USD", "LTC-USD", "ETH-USD"])
main_df.dropna(inplace=True)
main_df.reset_index(level=0, inplace=True)
train_x, test_x, train_y, test_y = sort_data(main_df)

trxf = open("/content/drive/My Drive/Colab Notebooks/train_x_file605",'wb')
pickle.dump(train_x, trxf)
trxf.close()

tryf = open("/content/drive/My Drive/Colab Notebooks/train_y_file605",'wb')
pickle.dump(train_y, tryf)
tryf.close()

texf = open("/content/drive/My Drive/Colab Notebooks/test_x_file605",'wb')
pickle.dump(test_x, texf)
texf.close()

teyf = open("/content/drive/My Drive/Colab Notebooks/test_y_file605",'wb')
pickle.dump(test_y, teyf)
teyf.close()

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tfy
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

# Commented out IPython magic to ensure Python compatibility.
import datetime, os
# %load_ext tensorboard

logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))



trxf = open("/content/drive/My Drive/Colab Notebooks/train_x_file605",'rb')
train_x = pickle.load(trxf)
trxf.close()

tryf = open("/content/drive/My Drive/Colab Notebooks/train_y_file605",'rb')
train_y = pickle.load(tryf)
tryf.close()

texf = open("/content/drive/My Drive/Colab Notebooks/test_x_file605",'rb')
test_x = pickle.load(texf)
texf.close()

teyf = open("/content/drive/My Drive/Colab Notebooks/test_y_file605",'rb')
test_y = pickle.load(teyf)
teyf.close()

model = create_network(train_x.shape[1:])
filepath = "RNN_Final-{epoch:02d}-{val_accuracy:.3f}"  # unique file name that will include the epoch and the validation acc for that epoch

checkpoint = ModelCheckpoint("/content/drive/My Drive/Colab Notebooks/models/1min/eth///{}.model".format(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')) # saves only the best ones

# Train model
history = model.fit(
    train_x, train_y,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=(test_x, test_y),
    callbacks=[checkpoint],
    shuffle=True,
)

# Score model
score = model.evaluate(test_x, test_y, verbose=0)